{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2608.001300 Machine Learning <br> Assignment #4 Na√Øve Bayes Classifier for Spam Filtering\n",
    "\n",
    "Copyright (C) Data Science Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. Written by Jangho Lee, May 2018\n",
    "\n",
    "**Note**: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem.\n",
    "\n",
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.  \n",
    "Once you have done **all parts**, run the *CollectSubmission.sh* script with your **student_id** as input argument. <br>\n",
    "This will produce a zipped file called *[student_id].zip*. Please submit this file on ETL. &nbsp;&nbsp; (Usage: ./*CollectSubmission.sh* &nbsp; student_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import collections\n",
    "import re\n",
    "import math\n",
    "import copy\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "\n",
    "# Set the data paths (training, test, and stop words)\n",
    "train_spam_dir = './data/train/spam'\n",
    "train_ham_dir = './data/train/ham'\n",
    "test_spam_dir = './data/test/spam'\n",
    "test_ham_dir = './data/test/ham'\n",
    "\n",
    "stop_words_path = './data/stop_words.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare some variables to put the training/test data\n",
    "\n",
    "# Define an empty dictionary to put the emails\n",
    "train_data = {}\n",
    "test_data = {}\n",
    "\n",
    "# Define classes\n",
    "classes = ['ham', 'spam']\n",
    "\n",
    "# Define an empty dictionary to put conditional probability\n",
    "cond_prob = {}\n",
    "cond_prob['ham'] = {}\n",
    "cond_prob['spam'] = {}\n",
    "\n",
    "# Define an empty dictionary to put prior\n",
    "prior = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write down 'Document' class to store email instance\n",
    "class Document:\n",
    "    text = ''\n",
    "    word_frequency = {}\n",
    "    \n",
    "    # spam / ham\n",
    "    true_class = ''\n",
    "    learned_class = ''\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, text, word_count, true_class):\n",
    "        self.text = text\n",
    "        self.word_frequency = word_count\n",
    "                    \n",
    "        self.true_class = true_class\n",
    "        \n",
    "    # return email content\n",
    "    def get_text(self):\n",
    "        return self.text\n",
    "    \n",
    "    # return word frequency\n",
    "    def get_word_frequency(self):\n",
    "        return self.word_frequency\n",
    "    \n",
    "    # return true class\n",
    "    def get_true_class(self):\n",
    "        return self.true_class\n",
    "    \n",
    "    # return predicted class\n",
    "    def get_predicted_class(self):\n",
    "        return self.learned_class\n",
    "    \n",
    "    # set the prediction\n",
    "    def set_predicted_class(self, prediction):\n",
    "        self.learned_class = prediction\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all text files in the given dictionary and construct the dataset, D\n",
    "def generate_dataset(storage_dict, path, true_class):\n",
    "    \"\"\"\n",
    "    Input: storage_dict, path, true_class\n",
    "    (We defined the some dictionaries to put the data and each dictionary has the form \n",
    "    {dir_path: Document(text, bag_of_words(text), true_class)}\n",
    "    \"\"\"\n",
    "    dirs = os.listdir(path)\n",
    "    for dir in dirs:\n",
    "        dir_path = os.path.join(path, dir)\n",
    "        if os.path.isfile(dir_path):\n",
    "            # open with 'codecs' package in order to suppress the error\n",
    "            with codecs.open(dir_path, 'r', encoding='utf-8', errors='ignore') as text_file:\n",
    "                text = text_file.read()\n",
    "                \n",
    "                # update storage\n",
    "                storage_dict[dir_path] = Document(text, bag_of_words(text), true_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count frequency of each word in the text in the text files \n",
    "def bag_of_words(text):\n",
    "    \"\"\"\n",
    "    Input: text (email contents)\n",
    "    Return: ditionray (key: word, value: count)\n",
    "    \"\"\"\n",
    "    dic = {}\n",
    "    \n",
    "    text_list = text.split()\n",
    "    \n",
    "    for w in text_list:\n",
    "        if w in dic:\n",
    "            dic[w] = dic[w] + 1\n",
    "        else:\n",
    "            dic[w] = 1\n",
    "            \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the vocabulary of all the text in a data set\n",
    "def extract_vocabulary(data_set):\n",
    "    \"\"\"\n",
    "    Input: dictionary of data \n",
    "          (updated dictionary after generate_dataset and remove_stop_words functions)\n",
    "    Return: vocabulary list\n",
    "    \n",
    "    \"\"\"\n",
    "    all_text = ''\n",
    "    v = []\n",
    "    \n",
    "    # concatenate all texts\n",
    "    for data in data_set:\n",
    "        all_text = all_text+\" \"+data_set[data].get_text()\n",
    "    # create words list using 'bag_of_words' previously defined function\n",
    "    temp = bag_of_words(all_text)\n",
    "    \n",
    "    for t in temp:\n",
    "        v.append(t)\n",
    "        \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the stop words using the 'stop_words.txt'\n",
    "# Open the txt file and put each stop word into the stop words list\n",
    "def set_stop_words():\n",
    "    \n",
    "    stops = [] # stop words list\n",
    "    \n",
    "    # insert each word into stop words list\n",
    "    data_set = None\n",
    "    with codecs.open(stop_words_path, 'r', encoding='utf-8', errors='ignore') as text_file:\n",
    "        text = text_file.read()\n",
    "        stops = text.split()\n",
    "        \n",
    "    return stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words from data using the property of 'Document' class\n",
    "def remove_stop_words(stops, data_set):\n",
    "    \"\"\"\n",
    "    Input: list of stop words\n",
    "    Return: filtered out data set\n",
    "    \"\"\"\n",
    "    new_data = {}\n",
    "    # remove the stop words in email contents, iteratively\n",
    "    for data in data_set:\n",
    "        \n",
    "        rmv_word = data_set[data].get_text().split()\n",
    "        \n",
    "        temp = []\n",
    "        for r in rmv_word:\n",
    "            if r not in stops:\n",
    "                temp.append(r)\n",
    "                \n",
    "        txt = ''\n",
    "        for t in temp:\n",
    "            txt = txt +' '+ t + ' '\n",
    "        \n",
    "        new_data[data] = Document(txt, bag_of_words(txt), data_set[data].get_true_class()) \n",
    "        \n",
    "        #print(txt)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(training, priors, cond_prob, alpha=0.1):\n",
    "    \n",
    "    # the vocabulary of the training set\n",
    "    v = extract_vocabulary(training) # type: list\n",
    "    \n",
    "    # the number of documents\n",
    "    n = len(training)\n",
    "    \n",
    "    # for each class in classes (spam / ham)\n",
    "    for c in list(classes):\n",
    "        # n_c is number of documents with true class c\n",
    "        n_c = 0.0\n",
    "        \n",
    "        # text_concatenation of text of all docs in class (D, c)\n",
    "        text_c = \"\"\n",
    "        \n",
    "        for data in training:\n",
    "            if training[data].get_true_class() == c:\n",
    "                n_c = n_c+1\n",
    "                text_c = text_c +\" \"+ training[data].get_text()\n",
    "        \n",
    "        priors[c] = n_c/n\n",
    "        \n",
    "        # Count frequencies/tokens of each term in text_c in dictionary form (i.e. token : frequency)\n",
    "        token_freqs = bag_of_words(text_c)\n",
    "        \n",
    "        N = 0.\n",
    "        for t in token_freqs:\n",
    "            \n",
    "            N += token_freqs[t]\n",
    "        \n",
    "        # Calculate conditional probabilities for each token and sum using laplace smoothing and log-scale\n",
    "        for t in list(v):\n",
    "            if t in token_freqs:\n",
    "                cond_prob[c][t] = math.log10(float((token_freqs[t]+alpha)/(N+alpha*len(v))))\n",
    "            else:\n",
    "                cond_prob[c][t] = math.log10(float(alpha/(N+alpha*len(v))))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "def test(data_instance, priors, cond_prob):\n",
    "    score = {}\n",
    "    for c in classes:\n",
    "        score[c] = math.log10(float(priors[c]))\n",
    "        \n",
    "        # For each data instance, \n",
    "        # get the word frequency and calculate the conditional probability\n",
    "        # to compare the score (spam or not)\n",
    "        \n",
    "        freq = data_instance.get_word_frequency()\n",
    "        for f in freq:\n",
    "            if f in cond_prob[c]:\n",
    "                score[c] += (cond_prob[c][f]) * freq[f]\n",
    "        \n",
    "    if score[\"spam\"] > score[\"ham\"]:\n",
    "        return \"spam\"\n",
    "    else:\n",
    "        return \"ham\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "generate_dataset(train_data, train_spam_dir, classes[1])\n",
    "generate_dataset(train_data, train_ham_dir, classes[0])\n",
    "generate_dataset(test_data, test_spam_dir, classes[1])\n",
    "generate_dataset(test_data, test_ham_dir, classes[0])\n",
    "\n",
    "\n",
    "stop_words = set_stop_words() # type: list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "\n",
    "train_data = remove_stop_words(stop_words, train_data)\n",
    "test_data = remove_stop_words(stop_words, test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam filtering accuracy:\t\t\t96.0251%\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.02]\n",
    "\n",
    "for a in alphas:\n",
    "    # Train\n",
    "    train(train_data, prior, cond_prob, a)\n",
    "    # Count the corretly classified emails\n",
    "    correct_predictions = 0\n",
    "    for i in test_data:\n",
    "        # predict the class (spam or not)\n",
    "        pred = test(test_data[i], prior, cond_prob)\n",
    "        test_data[i].set_predicted_class(pred)\n",
    "    \n",
    "        # calculate the accuracy\n",
    "        if pred == test_data[i].get_true_class():\n",
    "            correct_predictions += 1\n",
    "\n",
    "    print(\"Spam filtering accuracy:\\t\\t\\t%.4f%%\" % (100.0 * float(correct_predictions) / float(len(test_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
